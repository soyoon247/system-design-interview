# 9. 웹 크롤러 설계
### 기본 알고리즘
    1. url이 가리키는 모든 웹 페이지를 다운로드
    2. 다운받은 웹 페이지에서 url을 추출
    3. 추출된 url들을 다운로드할 url 목록에 추가하여 과정을 반복

### 질문할 것
- 크롤러의 용도는?
- 얼마나 많은 ? 매달 10억개
- 수정된 웹페이지 고려?
- 저장 ? 어느 기간동안 저장?
- 중복된 컨텐츠는?

### 개략적 추정
- 10억개의 웹 페이지 다운로드
- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400 / 초
- 최대 QPS = 2 * QPS = 800
- 웹 페이지의 평균 크기를 500k 라고 가정했을 때, 10억 페이지 * 500k = 500TB / 월
- 5년간 보관한다고 하면 500TB * 12개월 * 5년 = 30PB

### 컴포넌트들
#### 도메인 이름 변환기 
웹 페이지를 다운받기 위해 URL을 IP 주소로 변환

#### 컨텐츠 파서
이상한 웹페이지는 문제를 일으키고, 공간을 차지하므로 적절하게 파싱한 뒤 검증해야 함

#### 중복 컨텐츠인가? 
웹 페이지의 해시값을 비교

#### 컨텐츠 저장소
- 데이터 양이 많으므로 디스크에 저장
- 인기 있는 컨텐츠는 메모리에 두어 접근 지연 시간 줄이기

#### URL 추출기
HTML 페이지를 파싱하여 링크를 골라냄

#### URL 필터
특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 나는 URL 등을 배제

#### 이미 방문한 URL?
블룸 필터, 해시테이블 등을 활용하여 이미 방문했으면 다시 처리하지 않도록 함

### 상세 설계
#### DFS vs BFS
DFS는 그래프 크기가 클 경우 어느 정도로 깊숙이 갈지 가늠하기 어렵기 때문에, 웹 크롤러는 보통 BFS를 사용

#### 문제점
    1. 같은 호스트에 속한 링크를 계속 다운받게 됨 -> 예의없음 
    2. url 간 우선순위를 두지 않는다.

#### 예의 갖추기
동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청해야 한다. 
같은 웹 사이트의 페이지를 다운받는 테스크는 시간차를 두고 실행하도록 **웹 사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지**한다. 

- 큐 라우터가 같은 호스트에 속한 url 은 언제나 같은 큐로 가도록 보장
- 큐 선택기는 해당 큐에서 나온 url을 지정된 작업 스레드에 전달

#### 우선순위
페이지 랭크, 트래픽 양, 갱신 빈도 등 다양한 척도가 있을 수 있음
순위 결정장치가 url의 우선순위를 계산한다. 
우선순위별로 큐가 할당된다. 
큐 선택기는 순위가 높은 큐에서 더 자주 꺼낸다. 

#### 전체 흐름
url 입력 -> 순위 결정 장치에 따라 큐에 들어감 -> 전면 큐 선택기는 우선순위가 높은 큐에서 자주 꺼낸다. -> 후면 큐 라우터는 매핑 테이블에 따라 호스트와 맞는 큐에 넣는다 -> 후면 큐 선택기는 지정된 작업 스레드에 넣는다. 

#### 저장
모두 메모리에 보관 -> 안정성x, 규모확장성x
모두 디스크에 저장 -> 속도가 느려 병목지점이 된다.
대부분의 url은 디스크에 두지만, IO 비용을 줄이기 위해 메모리 버퍼에 큐를 둔다. 버퍼에 있는 데이터는 주기적으로 디스크에 기록한다. 

#### Robots.txt (로봇 제외 프로토콜)
웹사이트가 크롤러와 소통하는 표준적 방법. 크롤링 전에 해당 파일에 있는 규칙을 확인할 것.

#### 성능 최적화
    1. 분산 크롤링 : 여러 스레드를 돌려 다운로드 처리
    2. 도메인 이름 변환 결과 캐시 : 동기로 실행되기 때문에 크롤러 병목이 된다. DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해놓고 크론 잡으로 주기적 갱신하도록 하여 개선할 수 있다. 
    3. 지역성 : 크롤링 서버가 크롤링 대상 서버와 지역적으로 가까우면 페이지 다운로드 시간은 줄어들 수 있다. 
    4. 타임아웃 : 최대 얼마나 기다릴지 미리 정해둔다. 

#### 안정성
부하를 분산하기 위해 안정 해시 사용

#### 확장성
새로운 모듈을 끼워 넣을 수 있도록 한다. (png 다운로더, url 추출기, 웹 모니터 등)


#### 문제 있는 콘텐츠 감지 및 회피
    1. 중복 콘텐츠 : 해시나 체크섬 사용
    2. 거미 덫 : 크롤러가 무한 루프에 빠짐. 만능 해결책은 없고, 사람이 수작업하는 방법이 있다..
    3. 데이터 노이즈 제외 처리


#### 기타
CSR 의 경우 동적으로 생성되는 링크는 발견되지 않아서, ssr 적용하기도 함. 










